# Финальный проект по курсу «Цифровая архивация современности»

Репозиторий посвящён [коллекции](https://disk.yandex.ru/d/GvmfT8Jwr1_A7w) сайтов научных журналов Факультета гуманитарных наук НИУ ВШЭ, собранной при помощи [wpull](https://github.com/ArchiveTeam/wpull). Здесь опубликованы метаданные каждого элемента коллекции, созданные с использованием инструмента [metawarc](https://github.com/datacoon/metawarc), базы данных, ссылки на дополнительные материалы, связанные с коллекцией. Кроме того, в репозитории приведены результаты анализа сайтов с помощью сервиса [ArchiveReady](https://archiveready.com/), определяющего индекс и показатели архивируемости сайтов. 

## Краткое описание коллекции

В коллекцию вошли сайты научных журналов, аффилированных с Факультетом гуманитарных наук НИУ ВШЭ, а именно:

 1. [«Социологическое обозрение»](https://sociologica.hse.ru/)
 2. [«Философия. Журнал Высшей школы экономики»](https://philosophy.hse.ru/)
 3. [«Философические письма. Русско-европейский диалог»](https://phillet.hse.ru/)
 4. [«History HSE»](https://history.hse.ru/)
 5. [«Логос»](https://www.logosjournal.ru/)

Вся коллекция содержит файлы следующих форматов: html, json, js, css, img/png (т.е. все для функционирования сайта), txt, pdf.

Важность сохранения этих сайтов и иных подобных заключается в необходимости создания дополнительных хранилищ научных статей во избежание полной потери доступа к ним по разным причинам. Кроме того, сами сайты содержат важные данные о журналах, которые могут использоваться для изучения периодических изданий вообще или только электронной периодики, а также данные, которые могут быть полезны при изучении истории интернета.

## Метод и технологии

При сборе и обработке коллекции применялись следующие инструменты:

 1. Библиотека **wpull**, доступная на Python версии ниже 3.7: она предназначена для создания локальных архивов сайтов по определённым параметрам. Для удобства в работе использовался скрипт массовой загрузки, позволяющий последовательно скачивать несколько сайтов, перечисленных в предварительно подготовленном csv-файле, без дополнительных вмешательств.
 2. Утилита командной строки **metawarc**, позволяющая получать метаданные warc-файлов и обрабатывать архивы на их основе.
 3. Сервис **ArchiveReady**, определяющий индекс архивируемости сайтов и подробно описывающий отдельные показатели их архивируемости по методике [CLEAR](https://purl.pt/24107/1/iPres2013_PDF/CLEAR%20a%20credible%20method%20to%20evaluate%20website%20archivability.pdf).

## Структура проекта

1. Отдельные **папки**, названные в соответствии с адресами сайтов, вошедших в коллекцию. Внутри папок:
	- **README.md** – описание сайта, результаты обработки его архива по перечисленным в разделе «Метод и технологии» методам
	- **[название сайта]_meta.jsonl** – метаданные архива, собранные с помощью **metawarc**
	- **metadata_errors.txt** – ошибки, возникшие в ходе сбора метаданных
	- **metawarc.db** – база данных, составленная автоматически на основе метаданных
 	- **replaywebpage.jpg** – скриншот архива, открытого на сайте [REPLAY WEBPAGE](https://replayweb.page/)
  	- **archiveready.jpg** – скриншот саммари анализа архивируемости архива, проведённого при помощи сервиса **ArchiveReady** 
2. **README.md** – описание проекта

## Контакты

Автор проекта – [Йолдуз Хаертдинова](https://github.com/haertdinova)

## Условия использования

Материалы проекта (за исключением архивов и содержимого сайтов) могут использоваться и распространяться в соответствии с лицензией [Creative Commons BY 4.0](https://creativecommons.org/licenses/by-sa/4.0/).
